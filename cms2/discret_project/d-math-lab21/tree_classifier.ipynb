{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Імплементація\n",
    "\n",
    "Для реалізації дерева рішень знадобилося 2 класи: клас вершини дерева та самого дерева. Вершина містить тільки 8 атрибутів та 1 метод, який визначає клас вершини. Також додатково було цікаво, яка виходить складність цього алгоритму, тож ми її порахували.\n",
    "\n",
    "Дерево рішень містить лише 2 атрибути та декілька методів.\n",
    "1. **gini** — обраховує індекс Джині для вершини за формулою $G = 1 - \\sum_{k=1}^{n}p_{k}^2$. Складність цього алгоритму O(NlogN).\n",
    "\n",
    "2. **split_data** — розбиває дані на ліве та праве піддерева. Цей метод перебирає усі можливі варіанти розбиття та знаходить оптимальний (індекс Джині найменший). Складність цієї функції _O(F * NlogN)_, де F кількість фіч, а N кількість рядків для тренування. Взагалі, цю функцію можна було написати в _O(N*F)_, проте ми скористалися іншим алгоритмом. У запропонованому було передбачено повний перебір усіх фіч та даних, проте ми спочатку сортуємо дані за зростанням, а потім беремо _threshold_ як середнє арифметичне двох сусідніх елементів. Також, всередині ми рахуємо індекс Джині, складність якого _O(NlogN)_. Кінцева складність  $O(F*N^2*log^2(N))$\n",
    "\n",
    "3. **build_tree** — рекурсивна функція для побудови дерева. Створює вершину на кожному рівні, ділить дані для лівого та правого піддерева та рекурсивно робить їх, поки не досягнути _max\\_depth_. Складність цього алгоритму без урахування складності вкладених функцій O(NlogN), проте з урахуванням усіх вкладених функцій складність $O(F*N^3*log^3(N))$ у найгіршому випадку (якщо _max\\_depth_ не передано).\n",
    "4. **fit** — обгортка для методу build_tree.\n",
    "\n",
    "5. **predict_one** — отримує на вхід один екземпляр класу та за деревом \"вгадує\" клас. O(logN)\n",
    "\n",
    "6. **predict** — \"вгадує\" класи цілого списку. O(NlogN)\n",
    "\n",
    "7. **evaluate** — показує точність передбачення."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Decision Tree Classifier \"\"\"\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\" Node for a decision tree \"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, gini: float):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.gini = gini\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0.0\n",
    "\n",
    "        self.left: Node | None = None\n",
    "        self.right: Node | None = None\n",
    "\n",
    "        self.class_number: int | None = None\n",
    "\n",
    "    def detect_class(self):\n",
    "        \"\"\" Detect to which class node is \"\"\"\n",
    "        self.class_number = np.bincount(self.y).argmax()\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\" Decision tree \"\"\"\n",
    "    def __init__(self, max_depth: int | None = None):\n",
    "        \"\"\"Decision tree\n",
    "\n",
    "        Args:\n",
    "            max_depth (int | None, optional): max height of the tree. 0 or None cancels max height. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def gini(self, classes: np.ndarray) -> float:\n",
    "        \"\"\"A Gini score gives an idea of how good a split is by how mixed the\n",
    "        classes are in the two groups created by the split.\n",
    "        \n",
    "        A perfect separation results in a Gini score of 0,\n",
    "        whereas the worst case split that results in 50/50\n",
    "        classes in each group result in a Gini score of 0.5\n",
    "        (for a 2 class problem)\n",
    "        \n",
    "        Args:\n",
    "            classes (np.ndarray): list of used classes\n",
    "\n",
    "        Returns:\n",
    "            float: gini index\n",
    "\n",
    "        >>> Tree = DecisionTreeClassifier()\n",
    "        >>> Tree.gini(np.array([1, 2, 3, 2, 1]))\n",
    "        0.6399999999999999\n",
    "        \"\"\"\n",
    "        gini_sum = 0\n",
    "        number_of_classes = len(classes)\n",
    "\n",
    "        for group_class in np.unique(classes, return_counts=True)[1]:\n",
    "            gini_sum += (group_class / number_of_classes) ** 2\n",
    "\n",
    "        return 1 - gini_sum\n",
    "\n",
    "    def split_data(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float, float]:\n",
    "        \"\"\"Test all the possible splits\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): training data\n",
    "            y (np.ndarray): training answers\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, float, float]: index of feature, threshold and gini\n",
    "\n",
    "        >>> Tree = DecisionTreeClassifier()\n",
    "        >>> Tree.split_data(\n",
    "        ...     np.array([[10, 7], [2, 10], [5, 7]]),\n",
    "        ...     np.array([1, 0, 1])\n",
    "        ... )\n",
    "        (0, 3.5, 0.0)\n",
    "        \"\"\"\n",
    "        number_of_features = len(X[0])\n",
    "        number_of_classes = y.size\n",
    "\n",
    "        index = 0\n",
    "        threshold = 0.0\n",
    "        lowest_gini = np.Inf\n",
    "\n",
    "        # for all features\n",
    "        for class_idx in range(number_of_features):\n",
    "            # create heapq of feature column\n",
    "            active_group = []\n",
    "\n",
    "            for feature in X:\n",
    "                element = feature[class_idx]\n",
    "                heapq.heappush(active_group, element)\n",
    "\n",
    "            # we have to get mean of two neighbor elements\n",
    "            # 1 and 2, 2 and 3 and so on...\n",
    "            # so we need len(active_group) - 1 iterations\n",
    "            for _ in range(len(active_group) - 1):\n",
    "                # mean of two smallest elements\n",
    "                new_threshold = sum(heapq.nsmallest(2, active_group)) / 2\n",
    "                heapq.heappop(active_group)\n",
    "\n",
    "                # divide by left and right tree info\n",
    "                under_threshold = X[:, class_idx] < new_threshold\n",
    "                left_tree_y = y[under_threshold]\n",
    "                right_tree_y = y[~under_threshold]\n",
    "\n",
    "                # calc gini for children\n",
    "                left_gini = self.gini(left_tree_y)\n",
    "                right_gini = self.gini(right_tree_y)\n",
    "\n",
    "                left_nodes_count = len(left_tree_y)\n",
    "\n",
    "                # gini for this node\n",
    "                # i/m * Gini_left + (m-i)/m * Gini_right\n",
    "                gini = left_gini * (left_nodes_count / number_of_classes) +\\\n",
    "                    right_gini * (1 - (left_nodes_count / number_of_classes))\n",
    "\n",
    "                if gini < lowest_gini:\n",
    "                    lowest_gini = gini\n",
    "                    index = class_idx\n",
    "                    threshold = new_threshold\n",
    "\n",
    "        return index, threshold, lowest_gini\n",
    "\n",
    "    def build_tree(self, X: np.ndarray, y: np.ndarray, depth=0) -> Node | None:\n",
    "        \"\"\"create a root node\n",
    "        recursively split until max depth is not exceeded\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): training data\n",
    "            y (np.ndarray): training answers\n",
    "            depth (int, optional): max depth of tree. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            Node | None: root node of decision tree\n",
    "        \"\"\"\n",
    "        if self.max_depth and depth > self.max_depth:\n",
    "            return None\n",
    "\n",
    "        index, threshold, gini = self.split_data(X, y)\n",
    "\n",
    "        if index is None:\n",
    "            return None\n",
    "\n",
    "        # current Node\n",
    "        node = Node(X, y, gini)\n",
    "        node.feature_index = index\n",
    "        node.threshold = threshold\n",
    "        node.detect_class()\n",
    "\n",
    "        under_threshold = X[:, index] < threshold\n",
    "\n",
    "        # left child data\n",
    "        left_X = X[under_threshold]\n",
    "        left_y = y[under_threshold]\n",
    "\n",
    "        # right child data\n",
    "        right_X = X[~under_threshold]\n",
    "        right_y = y[~under_threshold]\n",
    "\n",
    "        # can't divide on left and right\n",
    "        if right_y.size == 0 or left_y.size == 0:\n",
    "            return node\n",
    "\n",
    "        node.left = self.build_tree(left_X, left_y, depth=depth + 1)\n",
    "        node.right = self.build_tree(right_X, right_y, depth=depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"basically wrapper for build tree / train\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): training data\n",
    "            y (np.ndarray): training answers\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict_one(self, test: np.ndarray) -> int | None:\n",
    "        \"\"\"Predict which class will test data have\n",
    "\n",
    "        Args:\n",
    "            test (np.ndarray): test data\n",
    "\n",
    "        Returns:\n",
    "            int: class index\n",
    "        \"\"\"\n",
    "        root = self.root\n",
    "\n",
    "        if root is None:\n",
    "            print(\"Train your decision tree at first\")\n",
    "            return None\n",
    "\n",
    "        while True:\n",
    "            feature = root.feature_index\n",
    "\n",
    "            if test[feature] < root.threshold:\n",
    "                if root.left is None:\n",
    "                    return root.class_number\n",
    "\n",
    "                root = root.left\n",
    "            else:\n",
    "                if root.right is None:\n",
    "                    return root.class_number\n",
    "\n",
    "                root = root.right\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> list[int | None]:\n",
    "        \"\"\"traverse the tree while there is a child\n",
    "        and return the predicted class for it, \n",
    "        note that X_test can be a single sample or a batch\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray): test data\n",
    "\n",
    "        Returns:\n",
    "            list[int | None]: list of classes\n",
    "        \"\"\"\n",
    "        return [self.predict_one(test) for test in X_test]\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> float:\n",
    "        \"\"\"return accuracy\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray): test data\n",
    "            y_test (np.ndarray): answers\n",
    "\n",
    "        Returns:\n",
    "            float: accuracy of prediction\n",
    "        \"\"\"\n",
    "        return sum(self.predict(X_test) == y_test) / len(y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приклад використання\n",
    "Для використання зійде будь-який датасет, який за структурою схожий на iris. Тут для прикладу дано 3: iris, wine та breast_cancer. Усі вони різного розміру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # (150, 4)\n",
    "from sklearn.datasets import load_wine # (178 13)\n",
    "from sklearn.datasets import load_breast_cancer # (569, 30)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вибирайте датасет, який найбільше подобається"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of samples. Size: (178, 13)\n",
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]]\n",
      "\n",
      "Unique classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Dataset. Choose one you like the most\n",
    "# X, y = load_iris(return_X_y=True) # (150, 4)\n",
    "X, y = load_wine(return_X_y=True) # (178 13)\n",
    "# X, y = load_breast_cancer(return_X_y=True) # (569, 30)\n",
    "\n",
    "print(\"Example of samples. Size:\", X.shape)\n",
    "print(X[:2])\n",
    "\n",
    "print()\n",
    "print(\"Unique classes:\", np.unique(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ділимо дані для тренування та тестування випадковим чином у пропорції 80/20 (з цим числом можна побавитися)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataset to training and test data\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створюємо екземпляр дерева рішень, тренуємо його та виводимо точність передбачень. 10 — максимальна висота дерева. З нею також можна погратися. 0 або None скасовує максимальну висоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [0, 2, 2, 2, 1, 2, 1, 0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 2, 2, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 2, 0]\n",
      "Accuracy: 88.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(10)\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(f\"Predicted classes: {tree.predict(X_test)}\")\n",
    "print(f\"Accuracy: {tree.evaluate(X_test, y_test) * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f79c9b16f38ffcb775a40230065153e5698d2449f4affda733ae3cb98dd9ffb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
